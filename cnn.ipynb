{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e38931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple as nt\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar10 \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "\n",
    "val_samples=5000\n",
    "\n",
    "Data = nt(\"Data\", \"x_train y_train x_valid y_valid x_test y_test\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a688ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test)= cifar10.load_data ()\n",
    "data = Data(x_train, y_train, None, None, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(data):\n",
    "    images_to_show = 36\n",
    "    per_row = 12\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    for i in range(images_to_show):\n",
    "        pos = (i // per_row, ((i % per_row) + per_row) % per_row)\n",
    "        ax = plt.subplot2grid((int(images_to_show / per_row), per_row), pos, xticks=[], yticks=[])\n",
    "        ax.imshow(np.squeeze(data.x_train[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88281b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A chart showing how the accuracy for the training and tests sets evolved\n",
    "def visualize_training(cifar10):\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot the accuracy for the training and tests sets\n",
    "    ax1.plot(cifar10.history['accuracy'])\n",
    "    ax1.plot(cifar10.history['val_accuracy'])\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "    # Plot the training vs validation loss\n",
    "    ax2.plot(cifar10.history['loss'])\n",
    "    ax2.plot(cifar10.history['val_loss'])\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "    # Adjust the spacing between the subplots\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "visualize_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "categories = len(np.unique(data.y_train))\n",
    "print(\"Shape of x_train pre-processing: \", data.x_train.shape)\n",
    "print(\"Shape of y_train pre-processing: \", data.y_train.shape)\n",
    "processed_data = cnn_preprocess(data, categories)\n",
    "print(\"Shape of x_train post-processing: \", processed_data.x_train.shape)\n",
    "print(\"Shape of y_train post-processing: \", processed_data.y_train.shape)\n",
    "print(\"Shape of x_valid post-processing: \", processed_data.x_valid.shape)\n",
    "print(\"Shape of y_valid post-processing: \", processed_data.y_valid.shape)\n",
    "print(\"Shape of x_test post-processing: \", processed_data.x_test.shape)\n",
    "print(\"Shape of y_test post-processing: \", processed_data.y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6699aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**CNN***CNN***CNN***CNN**#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2062a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "def cnn_preprocess(data, categories):\n",
    "    \n",
    "    # Z-score normalization of training data\n",
    "    mean = np.mean(data.x_train, axis=(0,1,2,3))\n",
    "    std = np.std(data.x_train, axis=(0,1,2,3))\n",
    "    x_train = ((data.x_train - mean) / (std + 1e-7)).astype(\"float32\")\n",
    "\n",
    "    # Z-score normalization of test data\n",
    "    mean = np.mean(data.x_test, axis=(0,1,2,3))\n",
    "    std = np.std(data.x_test, axis=(0,1,2,3))\n",
    "    x_test = ((data.x_test - mean) / (std + 1e-7)).astype(\"float32\")\n",
    "\n",
    "    y_train = to_categorical(data.y_train, categories)\n",
    "    y_test = to_categorical(data.y_test, categories)    \n",
    "    return Data(x_train[val_samples:], y_train[val_samples:],\n",
    "                x_train[:val_samples], y_train[:val_samples],\n",
    "                x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51060d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration parameters\n",
    "start_lr = 0.001\n",
    "exp_decay = 0.001\n",
    "\n",
    "# Define the scheduling function\n",
    "def schedule(epoch):\n",
    "  def lr(epoch, start_lr, exp_decay):\n",
    "    return start_lr * math.exp(-exp_decay*epoch)\n",
    "  return lr(epoch, start_lr, exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(data, categories):\n",
    "    # Create model architecture\n",
    "    weight_decay = 1e-4\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=3, padding=\"same\", activation=\"elu\",\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay), input_shape=data.x_train.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, kernel_size=3, padding=\"same\", activation=\"elu\",\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"elu\",\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"elu\",\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\", activation=\"elu\",\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\", activation=\"elu\",\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(categories, activation=\"softmax\"))\n",
    "    \n",
    "    optimized_rmsprop = RMSprop(learning_rate=0.001,decay=1e-6)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimized_rmsprop, metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68204db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for cnn\n",
    "cnn_processed_data = cnn_preprocess(data, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bca41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cnn\n",
    "cnn = build_cnn(cnn_processed_data, categories)\n",
    "print(\"CNN architecture:\")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.15, height_shift_range=0.15, horizontal_flip=True)\n",
    "datagen.fit(cnn_processed_data.x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the optimized cnn\n",
    "batch_size = 64\n",
    "cnn_path_best = \"weights/cifar10_cnn_best.hdf5\"\n",
    "checkpointer_cnn = ModelCheckpoint(cnn_path_best, verbose=1, save_best_only=True)\n",
    "cifar10_cnn = cnn.fit(datagen.flow(cnn_processed_data.x_train, cnn_processed_data.y_train, batch_size=batch_size),\n",
    "                                                 steps_per_epoch=cnn_processed_data.x_train.shape[0] // batch_size, epochs=200,\n",
    "                                                 verbose=0,validation_data=(cnn_processed_data.x_valid, cnn_processed_data.y_valid),\n",
    "                                                 callbacks=[checkpointer_cnn, LearningRateScheduler(schedule),\n",
    "                                                            EarlyStopping(min_delta=0.001, patience=30)])\n",
    "cnn.load_weights(cnn_path_best)\n",
    "score_cnn = cnn.evaluate(cnn_processed_data.x_test, cnn_processed_data.y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training(cifar10_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fa122",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy cnn: {0:.2f}%\".format(score_cnn[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**MLP***MLP***MLP***MLP**#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "categories = len(np.unique(data.y_train))\n",
    "print(\"Shape of x_train pre-processing: \", data.x_train.shape)\n",
    "print(\"Shape of y_train pre-processing: \", data.y_train.shape)\n",
    "processed_data = mlp_preprocess(data, categories)\n",
    "print(\"Shape of x_train post-processing: \", processed_data.x_train.shape)\n",
    "print(\"Shape of y_train post-processing: \", processed_data.y_train.shape)\n",
    "print(\"Shape of x_valid post-processing: \", processed_data.x_valid.shape)\n",
    "print(\"Shape of y_valid post-processing: \", processed_data.y_valid.shape)\n",
    "print(\"Shape of x_test post-processing: \", processed_data.x_test.shape)\n",
    "print(\"Shape of y_test post-processing: \", processed_data.y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7668f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_preprocess(data, categories):\n",
    "    x_train = data.x_train.astype(\"float32\") / 255\n",
    "    x_test = data.x_test.astype(\"float32\") / 255\n",
    "    y_train = to_categorical(data.y_train, categories)\n",
    "    y_test = to_categorical(data.y_test, categories)    \n",
    "    return Data(x_train[5000:], y_train[5000:],\n",
    "                x_train[:5000], y_train[:5000],\n",
    "                x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27eb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(data, categories):\n",
    "    # Create model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=data.x_train.shape[1:]))\n",
    "    model.add(Dense(1000, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(categories, activation=\"softmax\"))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b407df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mlp\n",
    "mlp = build_mlp(processed_data, categories)\n",
    "print(\"MLP architecture:\")\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ef074",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_weights_path = \"weights/cifar10_mlp_best.hdf5\"\n",
    "# Train the mlp\n",
    "checkpointer_mlp = ModelCheckpoint(filepath=mlp_weights_path, verbose=0, save_best_only=True)\n",
    "cifar10_mlp = mlp.fit(processed_data.x_train, processed_data.y_train, batch_size=32, \n",
    "                   epochs=100, validation_data=(processed_data.x_valid, processed_data.y_valid),\n",
    "                   callbacks=[checkpointer_mlp], shuffle=True)\n",
    "mlp.load_weights(mlp_weights_path)\n",
    "score_mlp = mlp.evaluate(processed_data.x_test, processed_data.y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training(cifar10_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy mlp: {0:.2f}%\".format(score_mlp[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4c90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
